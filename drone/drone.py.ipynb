{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "''' python 3.13 no mediapipe'''\n",
    "'''\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# -----------------------------\n",
    "# Dummy Drone Control Functions\n",
    "# -----------------------------\n",
    "def move_left():\n",
    "    print(\"ðŸ”µ DRONE COMMAND: LEFT\")\n",
    "\n",
    "def move_right():\n",
    "    print(\"ðŸ”´ DRONE COMMAND: RIGHT\")\n",
    "\n",
    "def move_up():\n",
    "    print(\"ðŸŸ¢ DRONE COMMAND: UP\")\n",
    "\n",
    "def move_down():\n",
    "    print(\"ðŸŸ¡ DRONE COMMAND: DOWN\")\n",
    "\n",
    "def hover():\n",
    "    print(\"âšª DRONE COMMAND: HOVER\")\n",
    "\n",
    "def stop():\n",
    "    print(\"ðŸ”´ DRONE COMMAND: STOP\")\n",
    "\n",
    "# -----------------------------\n",
    "# Improved Gesture Recognition\n",
    "# -----------------------------\n",
    "class GestureRecognizer:\n",
    "    def __init__(self):\n",
    "        # Use KNN background subtractor (more robust)\n",
    "        self.bg_subtractor = cv2.createBackgroundSubtractorKNN(\n",
    "            history=200,\n",
    "            dist2Threshold=400.0,\n",
    "            detectShadows=False\n",
    "        )\n",
    "\n",
    "        # Store background frames for better calibration\n",
    "        self.bg_frames = []\n",
    "        self.bg_frames_count = 0\n",
    "        self.bg_calibration_frames = 60  # More frames = better calibration\n",
    "        self.is_calibrated = False\n",
    "\n",
    "        # Running average for stability\n",
    "        self.avg_background = None\n",
    "\n",
    "        # Stability parameters\n",
    "        self.gesture_buffer = deque(maxlen=25)  # Longer buffer\n",
    "        self.confirmation_threshold = 18  # Need 18/25 frames (72%)\n",
    "        self.last_command_time = 0\n",
    "        self.command_cooldown = 2.0\n",
    "\n",
    "        # Adaptive thresholds\n",
    "        self.min_hand_area = 3000\n",
    "        self.max_hand_area = 100000\n",
    "\n",
    "    def calibrate_background(self, frame):\n",
    "        \"\"\"Improved background calibration with frame collection\"\"\"\n",
    "        # Preprocess frame for better calibration\n",
    "        frame_processed = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "        # Store frames for averaging\n",
    "        if self.bg_frames_count < self.bg_calibration_frames:\n",
    "            self.bg_frames.append(frame_processed.astype(np.float32))\n",
    "            self.bg_frames_count += 1\n",
    "\n",
    "            # Feed to background subtractor with high learning rate\n",
    "            self.bg_subtractor.apply(frame_processed, learningRate=0.8)\n",
    "\n",
    "            return False\n",
    "\n",
    "        # After collecting all frames, compute average background\n",
    "        if self.avg_background is None and len(self.bg_frames) > 0:\n",
    "            self.avg_background = np.mean(self.bg_frames, axis=0).astype(np.uint8)\n",
    "            print(\"âœ… Background model created!\")\n",
    "\n",
    "        self.is_calibrated = True\n",
    "        return True\n",
    "\n",
    "    def preprocess(self, frame):\n",
    "        \"\"\"Enhanced preprocessing with multiple techniques\"\"\"\n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        frame_blur = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "\n",
    "        # Method 1: Background subtraction\n",
    "        fg_mask = self.bg_subtractor.apply(frame_blur, learningRate=0)\n",
    "\n",
    "        # Method 2: Frame differencing with average background\n",
    "        if self.avg_background is not None:\n",
    "            frame_diff = cv2.absdiff(frame_blur, self.avg_background)\n",
    "            gray_diff = cv2.cvtColor(frame_diff, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh_diff = cv2.threshold(gray_diff, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Combine both methods (intersection for better accuracy)\n",
    "            fg_mask = cv2.bitwise_and(fg_mask, thresh_diff)\n",
    "\n",
    "        # Heavy morphological operations for cleaner mask\n",
    "        kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
    "\n",
    "        # Remove small noise\n",
    "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel_open, iterations=2)\n",
    "\n",
    "        # Fill holes\n",
    "        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel_close, iterations=2)\n",
    "\n",
    "        # Additional noise reduction\n",
    "        fg_mask = cv2.medianBlur(fg_mask, 5)\n",
    "\n",
    "        # Final threshold\n",
    "        _, fg_mask = cv2.threshold(fg_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        return fg_mask\n",
    "\n",
    "    def find_hand_contour(self, mask):\n",
    "        \"\"\"Find hand contour with better validation\"\"\"\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if not contours:\n",
    "            return None\n",
    "\n",
    "        # Filter by area with adaptive thresholds\n",
    "        valid_contours = []\n",
    "        for c in contours:\n",
    "            area = cv2.contourArea(c)\n",
    "            if self.min_hand_area < area < self.max_hand_area:\n",
    "                valid_contours.append(c)\n",
    "\n",
    "        if not valid_contours:\n",
    "            return None\n",
    "\n",
    "        # Get largest valid contour\n",
    "        max_contour = max(valid_contours, key=cv2.contourArea)\n",
    "\n",
    "        # Additional validation: check aspect ratio\n",
    "        x, y, w, h = cv2.boundingRect(max_contour)\n",
    "        aspect_ratio = float(w) / h if h != 0 else 0\n",
    "\n",
    "        # Hand should have reasonable aspect ratio (0.4 to 2.5)\n",
    "        if 0.4 < aspect_ratio < 2.5:\n",
    "            return max_contour\n",
    "\n",
    "        return None\n",
    "\n",
    "    def recognize_gesture(self, contour, frame_width, frame_height):\n",
    "        \"\"\"\n",
    "        More accurate gesture recognition with adjusted zones\n",
    "\n",
    "        GESTURES:\n",
    "        - Hand on LEFT third â†’ LEFT\n",
    "        - Hand on RIGHT third â†’ RIGHT\n",
    "        - Hand on TOP third â†’ UP\n",
    "        - Hand in CENTER + SMALL â†’ STOP\n",
    "        - Hand in CENTER + LARGE â†’ HOVER\n",
    "        \"\"\"\n",
    "        if contour is None:\n",
    "            return \"none\"\n",
    "\n",
    "        # Get bounding box\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Calculate center of hand\n",
    "        cx = x + w // 2\n",
    "        cy = y + h // 2\n",
    "\n",
    "        # Calculate hand area\n",
    "        area = cv2.contourArea(contour)\n",
    "\n",
    "        # Define clearer zones (thirds)\n",
    "        left_boundary = frame_width * 0.33\n",
    "        right_boundary = frame_width * 0.67\n",
    "        top_boundary = frame_height * 0.33\n",
    "        bottom_boundary = frame_height * 0.67\n",
    "\n",
    "        # Adaptive size thresholds based on frame size\n",
    "        frame_area = frame_width * frame_height\n",
    "        small_threshold = frame_area * 0.025  # 2.5% of frame\n",
    "        large_threshold = frame_area * 0.055  # 5.5% of frame\n",
    "\n",
    "        # PRIORITY 1: Position-based gestures (easier to detect)\n",
    "        if cx < left_boundary:\n",
    "            return \"left\"\n",
    "\n",
    "        if cx > right_boundary:\n",
    "            return \"right\"\n",
    "\n",
    "        if cy < top_boundary:\n",
    "            return \"up\"\n",
    "\n",
    "        # PRIORITY 2: Center gestures (size-based)\n",
    "        if left_boundary <= cx <= right_boundary:\n",
    "            if cy >= top_boundary:  # Must be in center or bottom\n",
    "                if area < small_threshold:\n",
    "                    return \"stop\"  # Small = fist\n",
    "                elif area > large_threshold:\n",
    "                    return \"hover\"  # Large = open palm\n",
    "\n",
    "        return \"none\"\n",
    "\n",
    "    def get_stable_gesture(self, current_gesture):\n",
    "        \"\"\"\n",
    "        Return gesture only if it's been stable with weighted voting\n",
    "        Recent frames have more weight\n",
    "        \"\"\"\n",
    "        # Add current gesture to buffer\n",
    "        self.gesture_buffer.append(current_gesture)\n",
    "\n",
    "        # Need minimum frames\n",
    "        if len(self.gesture_buffer) < self.confirmation_threshold:\n",
    "            return \"none\"\n",
    "\n",
    "        # Count occurrences with recency bias\n",
    "        gesture_counts = {}\n",
    "        buffer_list = list(self.gesture_buffer)\n",
    "\n",
    "        for i, g in enumerate(buffer_list):\n",
    "            # Give more weight to recent frames (last 10 frames get 2x weight)\n",
    "            weight = 2 if i >= len(buffer_list) - 10 else 1\n",
    "            gesture_counts[g] = gesture_counts.get(g, 0) + weight\n",
    "\n",
    "        # Remove \"none\" from consideration unless it's dominant\n",
    "        if \"none\" in gesture_counts and len(gesture_counts) > 1:\n",
    "            none_weight = gesture_counts[\"none\"]\n",
    "            total_weight = sum(gesture_counts.values())\n",
    "            # Only keep \"none\" if it's more than 60% of total\n",
    "            if none_weight < total_weight * 0.6:\n",
    "                del gesture_counts[\"none\"]\n",
    "\n",
    "        if not gesture_counts:\n",
    "            return \"none\"\n",
    "\n",
    "        # Find most common gesture\n",
    "        most_common = max(gesture_counts, key=gesture_counts.get)\n",
    "\n",
    "        # Calculate what percentage of recent frames match\n",
    "        recent_frames = buffer_list[-10:]\n",
    "        recent_match_count = sum(1 for g in recent_frames if g == most_common)\n",
    "\n",
    "        # Need at least 70% consistency in recent frames\n",
    "        if recent_match_count >= 7:\n",
    "            return most_common\n",
    "\n",
    "        return \"none\"\n",
    "\n",
    "    def can_send_command(self):\n",
    "        \"\"\"Check if enough time has passed since last command\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_command_time >= self.command_cooldown:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def update_command_time(self):\n",
    "        \"\"\"Update the last command timestamp\"\"\"\n",
    "        self.last_command_time = time.time()\n",
    "\n",
    "    def get_cooldown_remaining(self):\n",
    "        \"\"\"Get remaining cooldown time\"\"\"\n",
    "        elapsed = time.time() - self.last_command_time\n",
    "        remaining = max(0, self.command_cooldown - elapsed)\n",
    "        return remaining\n",
    "\n",
    "# -----------------------------\n",
    "# Main Application\n",
    "# -----------------------------\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    recognizer = GestureRecognizer()\n",
    "    current_command = \"none\"\n",
    "\n",
    "    # Display options\n",
    "    show_rgb = True  # Toggle to show RGB instead of BGR\n",
    "    show_debug = True  # Toggle to show debug visualizations\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸš GESTURE DRONE CONTROLLER - ENHANCED VERSION ðŸš\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nðŸ“‹ CALIBRATION (IMPORTANT!):\")\n",
    "    print(\"   1. Position camera to see plain background\")\n",
    "    print(\"   2. Keep BOTH HANDS away from camera view\")\n",
    "    print(\"   3. Stay COMPLETELY STILL for 3-4 seconds\")\n",
    "    print(\"   4. Wait for 'CALIBRATION COMPLETE' message\")\n",
    "    print(\"\\nâœ‹ SIMPLE GESTURES:\")\n",
    "    print(\"   ðŸ‘ˆ Move hand to LEFT third â†’ LEFT\")\n",
    "    print(\"   ðŸ‘‰ Move hand to RIGHT third â†’ RIGHT\")\n",
    "    print(\"   ðŸ‘† Move hand to TOP third â†’ UP\")\n",
    "    print(\"   âœŠ Center + SMALL hand (fist) â†’ STOP\")\n",
    "    print(\"   ðŸ–ï¸  Center + LARGE hand (open) â†’ HOVER\")\n",
    "    print(\"\\nðŸ’¡ TIPS:\")\n",
    "    print(\"   - Use plain/solid background for best results\")\n",
    "    print(\"   - Keep hand movements smooth and deliberate\")\n",
    "    print(\"   - Wait for stability bar to fill before moving\")\n",
    "    print(\"   - Press 'r' to recalibrate if detection is poor\")\n",
    "    print(\"\\nâš™ï¸  CONTROLS:\")\n",
    "    print(\"   'q' - Quit\")\n",
    "    print(\"   'r' - Recalibrate background\")\n",
    "    print(\"   't' - Toggle RGB/BGR display\")\n",
    "    print(\"   'd' - Toggle debug mask view\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Calibration phase\n",
    "    print(\"\\nâ³ STARTING CALIBRATION...\")\n",
    "    print(\"ðŸ“· Position yourself and stay still!\")\n",
    "    time.sleep(1)  # Give user a moment to read\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # ROI definition\n",
    "        roi_top, roi_bottom = 50, 430\n",
    "        roi_left, roi_right = 100, 540\n",
    "\n",
    "        # Extract ROI\n",
    "        roi = frame[roi_top:roi_bottom, roi_left:roi_right].copy()\n",
    "        roi_height, roi_width = roi.shape[:2]\n",
    "\n",
    "        # CALIBRATION PHASE\n",
    "        if not recognizer.is_calibrated:\n",
    "            is_complete = recognizer.calibrate_background(roi)\n",
    "            progress = (recognizer.bg_frames_count / recognizer.bg_calibration_frames) * 100\n",
    "\n",
    "            # Draw calibration UI\n",
    "            cv2.rectangle(frame, (roi_left, roi_top), (roi_right, roi_bottom), (0, 165, 255), 3)\n",
    "\n",
    "            # Instructions\n",
    "            cv2.putText(frame, \"CALIBRATING BACKGROUND...\", (width // 2 - 200, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 165, 255), 2)\n",
    "            cv2.putText(frame, \"Keep hand OUT of the box!\", (roi_left + 40, roi_top - 50),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "            cv2.putText(frame, \"Stay still for best results\", (roi_left + 40, roi_top - 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Progress bar (large and centered)\n",
    "            bar_x = width // 2 - 200\n",
    "            bar_y = height // 2\n",
    "            bar_width = 400\n",
    "            bar_height = 40\n",
    "\n",
    "            # Background\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (100, 100, 100), 2)\n",
    "\n",
    "            # Fill\n",
    "            fill_width = int((progress / 100) * (bar_width - 4))\n",
    "            cv2.rectangle(frame, (bar_x + 2, bar_y + 2), (bar_x + 2 + fill_width, bar_y + bar_height - 2), (0, 255, 0), -1)\n",
    "\n",
    "            # Progress text\n",
    "            cv2.putText(frame, f\"{int(progress)}%\", (bar_x + bar_width // 2 - 30, bar_y + 28),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "            # Frame counter\n",
    "            cv2.putText(frame, f\"Frame {recognizer.bg_frames_count}/{recognizer.bg_calibration_frames}\",\n",
    "                       (bar_x, bar_y + bar_height + 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "\n",
    "            if is_complete:\n",
    "                # Show completion message\n",
    "                cv2.putText(frame, \"CALIBRATION COMPLETE!\", (width // 2 - 180, height // 2 - 60),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"You can now use gestures!\", (width // 2 - 150, height // 2 - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # GESTURE RECOGNITION PHASE\n",
    "        else:\n",
    "            # Process hand\n",
    "            mask = recognizer.preprocess(roi)\n",
    "            contour = recognizer.find_hand_contour(mask)\n",
    "\n",
    "            # Recognize gesture (raw)\n",
    "            raw_gesture = \"none\"\n",
    "            hand_area = 0\n",
    "            hand_center = (0, 0)\n",
    "            area_percentage = 0\n",
    "\n",
    "            if contour is not None:\n",
    "                # Draw contour with thick line\n",
    "                cv2.drawContours(roi, [contour], -1, (0, 255, 0), 3)\n",
    "\n",
    "                # Get hand info\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                hand_center = (x + w // 2, y + h // 2)\n",
    "                hand_area = cv2.contourArea(contour)\n",
    "                frame_area = roi_width * roi_height\n",
    "                area_percentage = (hand_area / frame_area) * 100\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(roi, (x, y), (x + w, y + h), (255, 0, 255), 2)\n",
    "\n",
    "                # Draw center point (large)\n",
    "                cv2.circle(roi, hand_center, 8, (0, 0, 255), -1)\n",
    "                cv2.circle(roi, hand_center, 10, (255, 255, 255), 2)\n",
    "\n",
    "                # Show size indicator\n",
    "                size_text = f\"{area_percentage:.1f}%\"\n",
    "                cv2.putText(roi, size_text, (x, y - 10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "                raw_gesture = recognizer.recognize_gesture(contour, roi_width, roi_height)\n",
    "\n",
    "            # Get stable gesture\n",
    "            stable_gesture = recognizer.get_stable_gesture(raw_gesture)\n",
    "\n",
    "            # Send command\n",
    "            if stable_gesture != \"none\" and stable_gesture != current_command:\n",
    "                if recognizer.can_send_command():\n",
    "                    current_command = stable_gesture\n",
    "                    recognizer.update_command_time()\n",
    "\n",
    "                    # Execute command\n",
    "                    if stable_gesture == \"left\":\n",
    "                        move_left()\n",
    "                    elif stable_gesture == \"right\":\n",
    "                        move_right()\n",
    "                    elif stable_gesture == \"up\":\n",
    "                        move_up()\n",
    "                    elif stable_gesture == \"hover\":\n",
    "                        hover()\n",
    "                    elif stable_gesture == \"stop\":\n",
    "                        stop()\n",
    "\n",
    "            # Check cooldown status\n",
    "            if recognizer.can_send_command():\n",
    "                roi_color = (0, 255, 0)  # Green = ready\n",
    "                status_text = \"âœ… READY\"\n",
    "            else:\n",
    "                roi_color = (0, 165, 255)  # Orange = cooldown\n",
    "                cooldown = recognizer.get_cooldown_remaining()\n",
    "                status_text = f\"â³ COOLDOWN: {cooldown:.1f}s\"\n",
    "\n",
    "            # Draw ROI box with thick border\n",
    "            cv2.rectangle(frame, (roi_left, roi_top), (roi_right, roi_bottom), roi_color, 4)\n",
    "\n",
    "            # Draw zone guides (clearer thirds)\n",
    "            left_line = roi_left + int(roi_width * 0.33)\n",
    "            right_line = roi_left + int(roi_width * 0.67)\n",
    "            top_line = roi_top + int(roi_height * 0.33)\n",
    "\n",
    "            # Vertical lines\n",
    "            cv2.line(frame, (left_line, roi_top), (left_line, roi_bottom), (255, 255, 255), 2)\n",
    "            cv2.line(frame, (right_line, roi_top), (right_line, roi_bottom), (255, 255, 255), 2)\n",
    "\n",
    "            # Horizontal line\n",
    "            cv2.line(frame, (roi_left, top_line), (roi_right, top_line), (255, 255, 255), 2)\n",
    "\n",
    "            # Zone labels (larger)\n",
    "            cv2.putText(frame, \"LEFT\", (left_line - 70, roi_top + 40),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, \"RIGHT\", (right_line + 20, roi_top + 40),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, \"UP\", (roi_left + roi_width // 2 - 20, top_line - 15),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "            # Center zone labels\n",
    "            center_x = roi_left + roi_width // 2\n",
    "            center_y = roi_top + int(roi_height * 0.60)\n",
    "            cv2.putText(frame, \"STOP (small)\", (center_x - 70, center_y),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "            cv2.putText(frame, \"HOVER (large)\", (center_x - 75, center_y + 25),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "\n",
    "            # Display current command (extra large)\n",
    "            cmd_color = (0, 255, 0) if current_command != \"none\" else (100, 100, 100)\n",
    "            cv2.putText(frame, f\"CMD: {current_command.upper()}\", (20, 60),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, cmd_color, 4)\n",
    "\n",
    "            # Display raw detection\n",
    "            cv2.putText(frame, f\"Detect: {raw_gesture.upper()}\", (20, 110),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "            # Display hand info\n",
    "            info_y = 150\n",
    "            cv2.putText(frame, f\"Area: {int(hand_area)} ({area_percentage:.1f}%)\", (20, info_y),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "            cv2.putText(frame, f\"Center: ({hand_center[0]}, {hand_center[1]})\", (20, info_y + 25),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "\n",
    "            # Status (large)\n",
    "            cv2.putText(frame, status_text, (20, height - 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, roi_color, 2)\n",
    "\n",
    "            # Enhanced stability bar\n",
    "            bar_x = width - 230\n",
    "            bar_y = 20\n",
    "            bar_w = 220\n",
    "            bar_h = 50\n",
    "\n",
    "            # Background\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_w, bar_y + bar_h), (50, 50, 50), -1)\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_w, bar_y + bar_h), (255, 255, 255), 2)\n",
    "\n",
    "            # Calculate stability\n",
    "            buffer_list = list(recognizer.gesture_buffer)\n",
    "            if raw_gesture != \"none\" and buffer_list:\n",
    "                stable_count = sum(1 for g in buffer_list if g == raw_gesture)\n",
    "                stability_percent = (stable_count / len(buffer_list)) * 100\n",
    "\n",
    "                # Color based on stability\n",
    "                if stable_count >= recognizer.confirmation_threshold:\n",
    "                    bar_color = (0, 255, 0)  # Green = confirmed\n",
    "                elif stable_count >= 10:\n",
    "                    bar_color = (0, 255, 255)  # Yellow = getting there\n",
    "                else:\n",
    "                    bar_color = (0, 165, 255)  # Orange = unstable\n",
    "\n",
    "                # Fill bar\n",
    "                fill_w = int((stable_count / 25) * (bar_w - 8))\n",
    "                cv2.rectangle(frame, (bar_x + 4, bar_y + 4), (bar_x + 4 + fill_w, bar_y + bar_h - 4), bar_color, -1)\n",
    "\n",
    "                # Text\n",
    "                cv2.putText(frame, f\"Stability: {stable_count}/25\", (bar_x + 10, bar_y + 23),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, f\"{int(stability_percent)}%\", (bar_x + 10, bar_y + 42),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            else:\n",
    "                cv2.putText(frame, \"No hand detected\", (bar_x + 20, bar_y + 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)\n",
    "\n",
    "            # Debug view - show mask (larger)\n",
    "            if show_debug:\n",
    "                mask_small = cv2.resize(mask, (200, 200))\n",
    "                mask_bgr = cv2.cvtColor(mask_small, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "                # Add border\n",
    "                cv2.rectangle(mask_bgr, (0, 0), (199, 199), (255, 255, 255), 2)\n",
    "\n",
    "                frame[height-210:height-10, 10:210] = mask_bgr\n",
    "                cv2.putText(frame, \"MASK VIEW\", (15, height - 215),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        # Convert to RGB if toggled\n",
    "        display_frame = frame.copy()\n",
    "        if show_rgb:\n",
    "            display_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            cv2.putText(display_frame, \"RGB Mode\", (width - 120, height - 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        cv2.imshow(\"Gesture Drone Control\", display_frame)\n",
    "\n",
    "        # Keyboard controls\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            # Recalibrate\n",
    "            print(\"\\nðŸ”„ RECALIBRATING... Keep hand away!\")\n",
    "            recognizer = GestureRecognizer()\n",
    "            current_command = \"none\"\n",
    "        elif key == ord('t'):\n",
    "            # Toggle RGB/BGR\n",
    "            show_rgb = not show_rgb\n",
    "            print(f\"Display mode: {'RGB' if show_rgb else 'BGR'}\")\n",
    "        elif key == ord('d'):\n",
    "            # Toggle debug\n",
    "            show_debug = not show_debug\n",
    "            print(f\"Debug view: {'ON' if show_debug else 'OFF'}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    '''"
   ],
   "id": "c942e1489784b44b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "''' python 3.11.9 + mediapipe'''\n",
    "'''\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# =========================================================\n",
    "# Dummy Drone Control Functions\n",
    "# =========================================================\n",
    "def move_left():\n",
    "    print(\"ðŸŸ¦ DRONE COMMAND: LEFT\")\n",
    "\n",
    "def move_right():\n",
    "    print(\"ðŸŸ¥ DRONE COMMAND: RIGHT\")\n",
    "\n",
    "def move_up():\n",
    "    print(\"ðŸŸ© DRONE COMMAND: UP\")\n",
    "\n",
    "def hover():\n",
    "    print(\"â¬œ DRONE COMMAND: HOVER\")\n",
    "\n",
    "def stop():\n",
    "    print(\"ðŸŸ¥ DRONE COMMAND: STOP\")\n",
    "\n",
    "def summersault():\n",
    "    print(\"ðŸ¤˜ DRONE COMMAND: SUMMERSAULT\")\n",
    "\n",
    "# =========================================================\n",
    "# Gesture Recognizer (MediaPipe Tasks API)\n",
    "# =========================================================\n",
    "class GestureRecognizer:\n",
    "    def __init__(self):\n",
    "        base_options = python.BaseOptions(\n",
    "            model_asset_path=\"hand_landmarker.task\"\n",
    "        )\n",
    "\n",
    "        options = vision.HandLandmarkerOptions(\n",
    "            base_options=base_options,\n",
    "            num_hands=1,\n",
    "            min_hand_detection_confidence=0.7,\n",
    "            min_hand_presence_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        )\n",
    "\n",
    "        self.detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "        # Temporal stability - optimized for faster response\n",
    "        self.buffer = deque(maxlen=15)\n",
    "        self.confirmation_threshold = 10\n",
    "\n",
    "        # Cooldown - reduced for sequence chaining\n",
    "        self.last_command_time = 0\n",
    "        self.cooldown = 0.3  # Shorter cooldown for faster chaining\n",
    "\n",
    "        # Sequence chaining\n",
    "        self.sequence = []\n",
    "        self.max_sequence_length = 4\n",
    "        self.sequence_timeout = 0.4  # 0.4s between commands to chain\n",
    "        self.last_gesture_time = 0\n",
    "        self.sequence_active = False\n",
    "\n",
    "    # -----------------------------\n",
    "    # Finger State Logic (FIXED)\n",
    "    # -----------------------------\n",
    "    def fingers_extended(self, lm):\n",
    "        \"\"\"\n",
    "        Returns [thumb, index, middle, ring, pinky] extended state.\n",
    "        Fixed: fingers point up when tip.y < pip.y (lower y = higher on screen)\n",
    "        \"\"\"\n",
    "        fingers = []\n",
    "\n",
    "        # Thumb: compare x-axis (special case for horizontal extension)\n",
    "        thumb_tip = lm[4]\n",
    "        thumb_ip = lm[3]\n",
    "        wrist = lm[0]\n",
    "\n",
    "        # Determine if hand is on left or right side\n",
    "        if thumb_tip.x < wrist.x:  # Hand on left side\n",
    "            thumb_extended = thumb_tip.x < thumb_ip.x\n",
    "        else:  # Hand on right side\n",
    "            thumb_extended = thumb_tip.x > thumb_ip.x\n",
    "\n",
    "        fingers.append(thumb_extended)\n",
    "\n",
    "        # Other fingers: tip above pip (tip.y < pip.y)\n",
    "        tip_ids = [8, 12, 16, 20]\n",
    "        pip_ids = [6, 10, 14, 18]\n",
    "\n",
    "        for tip, pip in zip(tip_ids, pip_ids):\n",
    "            fingers.append(lm[tip].y < lm[pip].y)\n",
    "\n",
    "        return fingers\n",
    "\n",
    "    # -----------------------------\n",
    "    # Gesture Recognition (IMPROVED)\n",
    "    # -----------------------------\n",
    "    def recognize(self, landmarks):\n",
    "        \"\"\"\n",
    "        Recognizes gestures with improved accuracy, especially for 'up'.\n",
    "        Uses angle-based detection for directional gestures.\n",
    "        \"\"\"\n",
    "        fingers = self.fingers_extended(landmarks)\n",
    "        thumb, index, middle, ring, pinky = fingers\n",
    "\n",
    "        wrist = landmarks[0]\n",
    "        index_tip = landmarks[8]\n",
    "\n",
    "        # Calculate displacement\n",
    "        dx = index_tip.x - wrist.x\n",
    "        dy = wrist.y - index_tip.y  # Positive when finger points up\n",
    "\n",
    "        # Rock horns ðŸ¤˜ (index and pinky extended, middle and ring folded)\n",
    "        if index and pinky and not middle and not ring:\n",
    "            return \"summersault\"\n",
    "\n",
    "        # Index-only gestures (pointing)\n",
    "        if index and not middle and not ring and not pinky:\n",
    "            # Calculate angle from horizontal\n",
    "            angle = np.arctan2(dy, abs(dx)) * 180 / np.pi\n",
    "\n",
    "            # UP: finger mostly vertical (>55 degrees from horizontal)\n",
    "            if angle > 55:\n",
    "                return \"up\"\n",
    "            # LEFT: finger pointing left with strong horizontal component\n",
    "            elif dx < -0.10 and angle < 45:\n",
    "                return \"left\"\n",
    "            # RIGHT: finger pointing right with strong horizontal component\n",
    "            elif dx > 0.10 and angle < 45:\n",
    "                return \"right\"\n",
    "            else:\n",
    "                # Default to up if pointing generally upward\n",
    "                return \"up\" if dy > 0.05 else \"none\"\n",
    "\n",
    "        # Open palm (all fingers extended)\n",
    "        if index and middle and ring and pinky:\n",
    "            return \"hover\"\n",
    "\n",
    "        # Fist (no fingers extended)\n",
    "        if not any(fingers):\n",
    "            return \"stop\"\n",
    "\n",
    "        return \"none\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Temporal Stability\n",
    "    # -----------------------------\n",
    "    def stable_gesture(self, gesture):\n",
    "        \"\"\"\n",
    "        Applies temporal smoothing to reduce jitter.\n",
    "        A gesture must appear consistently to be confirmed.\n",
    "        \"\"\"\n",
    "        self.buffer.append(gesture)\n",
    "\n",
    "        if len(self.buffer) < self.confirmation_threshold:\n",
    "            return \"none\"\n",
    "\n",
    "        # Count occurrences of each gesture\n",
    "        counts = {}\n",
    "        for g in self.buffer:\n",
    "            if g != \"none\":\n",
    "                counts[g] = counts.get(g, 0) + 1\n",
    "\n",
    "        if not counts:\n",
    "            return \"none\"\n",
    "\n",
    "        # Return gesture only if it appears enough times\n",
    "        best = max(counts, key=counts.get)\n",
    "        return best if counts[best] >= self.confirmation_threshold else \"none\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Cooldown Control\n",
    "    # -----------------------------\n",
    "    def can_send(self):\n",
    "        \"\"\"Check if enough time has passed since last command.\"\"\"\n",
    "        return time.time() - self.last_command_time >= self.cooldown\n",
    "\n",
    "    def mark_sent(self):\n",
    "        \"\"\"Mark that a command was just sent.\"\"\"\n",
    "        self.last_command_time = time.time()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Sequence Chaining\n",
    "    # -----------------------------\n",
    "    def add_to_sequence(self, gesture):\n",
    "        \"\"\"\n",
    "        Add a gesture to the sequence buffer.\n",
    "        If 0.4s passes without a new gesture, execute the sequence.\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Check if we should start a new sequence\n",
    "        if current_time - self.last_gesture_time > self.sequence_timeout:\n",
    "            # Timeout - execute current sequence if it exists\n",
    "            if self.sequence:\n",
    "                self.execute_sequence()\n",
    "            self.sequence = []\n",
    "\n",
    "        # Add gesture to sequence\n",
    "        if len(self.sequence) < self.max_sequence_length:\n",
    "            self.sequence.append(gesture)\n",
    "            self.last_gesture_time = current_time\n",
    "            self.sequence_active = True\n",
    "            return False  # Not ready to execute yet\n",
    "        else:\n",
    "            # Sequence is full, execute it\n",
    "            self.execute_sequence()\n",
    "            return True\n",
    "\n",
    "    def execute_sequence(self):\n",
    "        \"\"\"Execute the chained sequence of commands.\"\"\"\n",
    "        if not self.sequence:\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ðŸŽ¯ EXECUTING SEQUENCE: {' â†’ '.join([s.upper() for s in self.sequence])}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        for i, gesture in enumerate(self.sequence, 1):\n",
    "            print(f\"  [{i}/{len(self.sequence)}] \", end=\"\")\n",
    "            if gesture == \"left\":\n",
    "                move_left()\n",
    "            elif gesture == \"right\":\n",
    "                move_right()\n",
    "            elif gesture == \"up\":\n",
    "                move_up()\n",
    "            elif gesture == \"hover\":\n",
    "                hover()\n",
    "            elif gesture == \"stop\":\n",
    "                stop()\n",
    "            elif gesture == \"summersault\":\n",
    "                summersault()\n",
    "\n",
    "            # Delay between commands in sequence for smoother execution\n",
    "            if i < len(self.sequence):\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        # Clear sequence\n",
    "        self.sequence = []\n",
    "        self.sequence_active = False\n",
    "\n",
    "    def check_sequence_timeout(self):\n",
    "        \"\"\"Check if sequence has timed out and should be executed.\"\"\"\n",
    "        if self.sequence and time.time() - self.last_gesture_time > self.sequence_timeout:\n",
    "            self.execute_sequence()\n",
    "\n",
    "    def get_sequence_status(self):\n",
    "        \"\"\"Get current sequence information.\"\"\"\n",
    "        return {\n",
    "            'sequence': self.sequence.copy(),\n",
    "            'count': len(self.sequence),\n",
    "            'max': self.max_sequence_length,\n",
    "            'active': self.sequence_active\n",
    "        }\n",
    "\n",
    "# =========================================================\n",
    "# Main Application\n",
    "# =========================================================\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    recognizer = GestureRecognizer()\n",
    "    current_command = \"none\"\n",
    "\n",
    "    print(\"\\nðŸš MEDIAPIPE GESTURE DRONE CONTROLLER ðŸš\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Gestures:\")\n",
    "    print(\"  ðŸ‘† Index finger UP â†’ MOVE UP\")\n",
    "    print(\"  ðŸ‘ˆ Index finger LEFT â†’ MOVE LEFT\")\n",
    "    print(\"  ðŸ‘‰ Index finger RIGHT â†’ MOVE RIGHT\")\n",
    "    print(\"  ðŸ–  Open palm â†’ HOVER\")\n",
    "    print(\"  âœŠ  Fist â†’ STOP\")\n",
    "    print(\"  ðŸ¤˜ Rock horns â†’ SUMMERSAULT\")\n",
    "    print()\n",
    "    print(\"â›“ï¸  SEQUENCE CHAINING:\")\n",
    "    print(\"  Chain up to 4 maneuvers by performing gestures\")\n",
    "    print(\"  within 0.4s of each other. The sequence executes\")\n",
    "    print(\"  automatically after 0.4s or when 4 moves are chained.\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Press 'q' to quit\\n\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Flip frame for mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Define ROI (Region of Interest)\n",
    "        roi_x1, roi_y1 = 120, 60\n",
    "        roi_x2, roi_y2 = 520, 420\n",
    "        roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "        # Convert to RGB for MediaPipe\n",
    "        rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Create MediaPipe Image\n",
    "        mp_image = mp.Image(\n",
    "            image_format=mp.ImageFormat.SRGB,\n",
    "            data=rgb\n",
    "        )\n",
    "\n",
    "        # Detect hand landmarks\n",
    "        result = recognizer.detector.detect(mp_image)\n",
    "\n",
    "        raw_gesture = \"none\"\n",
    "\n",
    "        # Process detection results\n",
    "        if result.hand_landmarks:\n",
    "            landmarks = result.hand_landmarks[0]\n",
    "            raw_gesture = recognizer.recognize(landmarks)\n",
    "\n",
    "            # Draw hand landmarks on frame\n",
    "            for landmark in landmarks:\n",
    "                x = int(landmark.x * (roi_x2 - roi_x1)) + roi_x1\n",
    "                y = int(landmark.y * (roi_y2 - roi_y1)) + roi_y1\n",
    "                cv2.circle(frame, (x, y), 5, (0, 255, 255), -1)\n",
    "\n",
    "        # Apply temporal stability\n",
    "        stable = recognizer.stable_gesture(raw_gesture)\n",
    "\n",
    "        # Check if sequence has timed out\n",
    "        recognizer.check_sequence_timeout()\n",
    "\n",
    "        # Send command if stable\n",
    "        if stable != \"none\":\n",
    "            if recognizer.can_send():\n",
    "                # Only add if it's different from the last gesture in sequence\n",
    "                # or if sequence is empty\n",
    "                seq_status = recognizer.get_sequence_status()\n",
    "                last_in_seq = seq_status['sequence'][-1] if seq_status['sequence'] else None\n",
    "\n",
    "                if stable != last_in_seq:\n",
    "                    current_command = stable\n",
    "                    recognizer.mark_sent()\n",
    "                    # Add to sequence instead of executing immediately\n",
    "                    sequence_full = recognizer.add_to_sequence(stable)\n",
    "\n",
    "        # -----------------------------\n",
    "        # UI Rendering\n",
    "        # -----------------------------\n",
    "\n",
    "        # Get sequence status\n",
    "        seq_status = recognizer.get_sequence_status()\n",
    "\n",
    "        # Draw ROI rectangle\n",
    "        cv2.rectangle(frame, (roi_x1, roi_y1), (roi_x2, roi_y2), (0, 255, 0), 3)\n",
    "\n",
    "        # Draw background for text\n",
    "        cv2.rectangle(frame, (0, 0), (640, 150), (0, 0, 0), -1)\n",
    "\n",
    "        # Display raw gesture\n",
    "        cv2.putText(frame, f\"RAW: {raw_gesture.upper()}\",\n",
    "                    (20, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # Display current command (larger and colored)\n",
    "        color = (0, 255, 0) if current_command != \"none\" else (100, 100, 100)\n",
    "        cv2.putText(frame, f\"CMD: {current_command.upper()}\",\n",
    "                    (20, 90),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.3, color, 3)\n",
    "\n",
    "        # Display sequence chain\n",
    "        if seq_status['count'] > 0:\n",
    "            seq_text = \" â†’ \".join([s.upper() for s in seq_status['sequence']])\n",
    "            cv2.putText(frame, f\"CHAIN [{seq_status['count']}/{seq_status['max']}]: {seq_text}\",\n",
    "                        (20, 130),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "        # Display buffer status\n",
    "        buffer_fill = len(recognizer.buffer)\n",
    "        cv2.putText(frame, f\"Buffer: {buffer_fill}/{recognizer.confirmation_threshold}\",\n",
    "                    (450, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Gesture Drone Control (MediaPipe)\", frame)\n",
    "\n",
    "        # Exit on 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nâœ… Application closed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    '''"
   ],
   "id": "8c4811e005dc46d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T13:40:35.638562Z",
     "start_time": "2026-02-07T13:40:12.492029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "ALL-IN-ONE CNN GESTURE RECOGNITION SYSTEM\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import sqlite3\n",
    "import pickle\n",
    "import os\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# =========================================================\n",
    "# DATABASE MANAGER\n",
    "# =========================================================\n",
    "class GestureDatabase:\n",
    "    \"\"\"Manages gesture data storage and retrieval\"\"\"\n",
    "\n",
    "    def __init__(self, db_path=\"gesture_database.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.init_database()\n",
    "\n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize database tables\"\"\"\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        cursor = self.conn.cursor()\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS gesture_samples (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                gesture_name TEXT NOT NULL,\n",
    "                landmarks BLOB NOT NULL,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                hand_edness TEXT,\n",
    "                confidence REAL\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS gesture_labels (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                gesture_name TEXT UNIQUE NOT NULL,\n",
    "                description TEXT,\n",
    "                drone_command TEXT,\n",
    "                created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS training_sessions (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                model_path TEXT,\n",
    "                accuracy REAL,\n",
    "                loss REAL,\n",
    "                epochs INTEGER,\n",
    "                samples_count INTEGER,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_gesture_label(self, name, description, drone_command):\n",
    "        \"\"\"Add a new gesture label\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO gesture_labels (gesture_name, description, drone_command)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (name, description, drone_command))\n",
    "            self.conn.commit()\n",
    "            return True\n",
    "        except sqlite3.IntegrityError:\n",
    "            return False\n",
    "\n",
    "    def save_sample(self, gesture_name, landmarks, hand_edness=\"Right\", confidence=1.0):\n",
    "        \"\"\"Save a gesture sample to database\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        landmarks_blob = pickle.dumps(landmarks)\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO gesture_samples (gesture_name, landmarks, hand_edness, confidence)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (gesture_name, landmarks_blob, hand_edness, confidence))\n",
    "\n",
    "        self.conn.commit()\n",
    "\n",
    "    def get_samples(self, gesture_name=None):\n",
    "        \"\"\"Retrieve gesture samples\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "\n",
    "        if gesture_name:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT gesture_name, landmarks FROM gesture_samples\n",
    "                WHERE gesture_name = ?\n",
    "            \"\"\", (gesture_name,))\n",
    "        else:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT gesture_name, landmarks FROM gesture_samples\n",
    "            \"\"\")\n",
    "\n",
    "        samples = []\n",
    "        for row in cursor.fetchall():\n",
    "            name, landmarks_blob = row\n",
    "            landmarks = pickle.loads(landmarks_blob)\n",
    "            samples.append((name, landmarks))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def get_all_gesture_names(self):\n",
    "        \"\"\"Get list of all unique gesture names\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT DISTINCT gesture_name FROM gesture_samples ORDER BY gesture_name\")\n",
    "        return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    def get_sample_count(self):\n",
    "        \"\"\"Get count of samples per gesture\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT gesture_name, COUNT(*) as count\n",
    "            FROM gesture_samples\n",
    "            GROUP BY gesture_name\n",
    "            ORDER BY gesture_name\n",
    "        \"\"\")\n",
    "        return dict(cursor.fetchall())\n",
    "\n",
    "    def save_training_session(self, model_path, accuracy, loss, epochs, samples_count):\n",
    "        \"\"\"Save training session metadata\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO training_sessions (model_path, accuracy, loss, epochs, samples_count)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\", (model_path, accuracy, loss, epochs, samples_count))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def clear_samples(self, gesture_name=None):\n",
    "        \"\"\"Clear samples (for a specific gesture or all)\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        if gesture_name:\n",
    "            cursor.execute(\"DELETE FROM gesture_samples WHERE gesture_name = ?\", (gesture_name,))\n",
    "        else:\n",
    "            cursor.execute(\"DELETE FROM gesture_samples\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# =========================================================\n",
    "# CNN MODEL\n",
    "# =========================================================\n",
    "class GestureCNN:\n",
    "    \"\"\"CNN model for gesture classification\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, input_shape=(21, 3)):\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.model = None\n",
    "        self.label_encoder = {}\n",
    "        self.label_decoder = {}\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN architecture\"\"\"\n",
    "        model = models.Sequential([\n",
    "            layers.Flatten(input_shape=self.input_shape),\n",
    "\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def prepare_data(self, samples):\n",
    "        \"\"\"Prepare data from database samples for training\"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        unique_labels = sorted(list(set([name for name, _ in samples])))\n",
    "        self.label_encoder = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.label_decoder = {idx: label for label, idx in self.label_encoder.items()}\n",
    "\n",
    "        for gesture_name, landmarks in samples:\n",
    "            landmark_array = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])\n",
    "            X.append(landmark_array)\n",
    "            y.append(self.label_encoder[gesture_name])\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = to_categorical(y, num_classes=self.num_classes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train(self, X, y, epochs=50, validation_split=0.2, batch_size=32):\n",
    "        \"\"\"Train the CNN model\"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def save_model(self, model_path=\"gesture_cnn_model.h5\"):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        self.model.save(model_path)\n",
    "\n",
    "        with open(model_path.replace('.h5', '_labels.pkl'), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'encoder': self.label_encoder,\n",
    "                'decoder': self.label_decoder\n",
    "            }, f)\n",
    "\n",
    "    def load_model(self, model_path=\"gesture_cnn_model.h5\"):\n",
    "        \"\"\"Load trained model\"\"\"\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "\n",
    "        with open(model_path.replace('.h5', '_labels.pkl'), 'rb') as f:\n",
    "            labels = pickle.load(f)\n",
    "            self.label_encoder = labels['encoder']\n",
    "            self.label_decoder = labels['decoder']\n",
    "\n",
    "        self.num_classes = len(self.label_encoder)\n",
    "\n",
    "    def predict(self, landmarks):\n",
    "        \"\"\"Predict gesture from landmarks\"\"\"\n",
    "        landmark_array = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])\n",
    "        landmark_array = landmark_array.reshape(1, 21, 3)\n",
    "\n",
    "        predictions = self.model.predict(landmark_array, verbose=0)\n",
    "\n",
    "        class_idx = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][class_idx]\n",
    "        gesture_name = self.label_decoder[class_idx]\n",
    "\n",
    "        return gesture_name, confidence\n",
    "\n",
    "# =========================================================\n",
    "# GESTURE RECOGNIZER\n",
    "# =========================================================\n",
    "class HybridGestureRecognizer:\n",
    "    \"\"\"Combines MediaPipe detection with CNN classification\"\"\"\n",
    "\n",
    "    def __init__(self, use_cnn=True, model_path=None):\n",
    "        # MediaPipe hand detector\n",
    "        base_options = python.BaseOptions(\n",
    "            model_asset_path=\"hand_landmarker.task\"\n",
    "        )\n",
    "\n",
    "        options = vision.HandLandmarkerOptions(\n",
    "            base_options=base_options,\n",
    "            num_hands=1,\n",
    "            min_hand_detection_confidence=0.7,\n",
    "            min_hand_presence_confidence=0.7,\n",
    "            min_tracking_confidence=0.7\n",
    "        )\n",
    "\n",
    "        self.detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "        # CNN classifier\n",
    "        self.use_cnn = use_cnn\n",
    "        self.cnn = None\n",
    "\n",
    "        if use_cnn and model_path and os.path.exists(model_path):\n",
    "            self.cnn = GestureCNN(num_classes=6)\n",
    "            self.cnn.load_model(model_path)\n",
    "\n",
    "        # Stability buffer\n",
    "        self.buffer = deque(maxlen=15)\n",
    "        self.confirmation_threshold = 10\n",
    "\n",
    "        # Cooldown\n",
    "        self.last_command_time = 0\n",
    "        self.cooldown = 2.0\n",
    "\n",
    "    def detect_hand(self, frame):\n",
    "        \"\"\"Detect hand landmarks using MediaPipe\"\"\"\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "\n",
    "        result = self.detector.detect(mp_image)\n",
    "\n",
    "        if result.hand_landmarks:\n",
    "            return result.hand_landmarks[0]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def recognize_gesture(self, landmarks):\n",
    "        \"\"\"Recognize gesture using CNN\"\"\"\n",
    "        if self.use_cnn and self.cnn:\n",
    "            gesture_name, confidence = self.cnn.predict(landmarks)\n",
    "            return gesture_name, confidence\n",
    "        else:\n",
    "            return \"none\", 0.0\n",
    "\n",
    "    def stable_gesture(self, gesture):\n",
    "        \"\"\"Apply temporal smoothing\"\"\"\n",
    "        self.buffer.append(gesture)\n",
    "\n",
    "        if len(self.buffer) < self.confirmation_threshold:\n",
    "            return \"none\"\n",
    "\n",
    "        counts = {}\n",
    "        for g in self.buffer:\n",
    "            if g != \"none\":\n",
    "                counts[g] = counts.get(g, 0) + 1\n",
    "\n",
    "        if not counts:\n",
    "            return \"none\"\n",
    "\n",
    "        best = max(counts, key=counts.get)\n",
    "        return best if counts[best] >= self.confirmation_threshold else \"none\"\n",
    "\n",
    "    def can_send(self):\n",
    "        return time.time() - self.last_command_time >= self.cooldown\n",
    "\n",
    "    def mark_sent(self):\n",
    "        self.last_command_time = time.time()\n",
    "\n",
    "# =========================================================\n",
    "# DRONE COMMANDS\n",
    "# =========================================================\n",
    "def move_left():\n",
    "    print(\"ðŸ”µ DRONE COMMAND: LEFT\")\n",
    "\n",
    "def move_right():\n",
    "    print(\"ðŸ”´ DRONE COMMAND: RIGHT\")\n",
    "\n",
    "def move_up():\n",
    "    print(\"ðŸŸ¢ DRONE COMMAND: UP\")\n",
    "\n",
    "def hover():\n",
    "    print(\"âšª DRONE COMMAND: HOVER\")\n",
    "\n",
    "def stop():\n",
    "    print(\"ðŸ›‘ DRONE COMMAND: STOP\")\n",
    "\n",
    "def summersault():\n",
    "    print(\"ðŸ¤¸ DRONE COMMAND: SUMMERSAULT\")\n",
    "\n",
    "COMMAND_MAP = {\n",
    "    'left': move_left,\n",
    "    'right': move_right,\n",
    "    'up': move_up,\n",
    "    'hover': hover,\n",
    "    'stop': stop,\n",
    "    'summersault': summersault\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# DATA COLLECTION MODULE\n",
    "# =========================================================\n",
    "def collect_data():\n",
    "    \"\"\"Data collection interface\"\"\"\n",
    "    db = GestureDatabase()\n",
    "    recognizer = HybridGestureRecognizer(use_cnn=False)\n",
    "\n",
    "    # Setup gestures\n",
    "    gestures = {\n",
    "        '1': 'left',\n",
    "        '2': 'right',\n",
    "        '3': 'up',\n",
    "        '4': 'hover',\n",
    "        '5': 'stop',\n",
    "        '6': 'summersault'\n",
    "    }\n",
    "\n",
    "    # Add gesture labels\n",
    "    for name in gestures.values():\n",
    "        db.add_gesture_label(name, f'Gesture {name}', f'move_{name}')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    current_gesture = None\n",
    "    collecting = False\n",
    "    sample_count = 0\n",
    "    target_samples = 100\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š DATA COLLECTION MODE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nGestures:\")\n",
    "    for key, name in gestures.items():\n",
    "        print(f\"  {key} - {name.upper()}\")\n",
    "    print(\"\\nControls:\")\n",
    "    print(\"  1-6: Select gesture\")\n",
    "    print(\"  c: Stop collecting\")\n",
    "    print(\"  s: Show statistics\")\n",
    "    print(\"  q: Quit\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        roi_x1, roi_y1 = 120, 60\n",
    "        roi_x2, roi_y2 = 520, 420\n",
    "        roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "        landmarks = recognizer.detect_hand(roi)\n",
    "\n",
    "        if landmarks:\n",
    "            for landmark in landmarks:\n",
    "                x = int(landmark.x * (roi_x2 - roi_x1)) + roi_x1\n",
    "                y = int(landmark.y * (roi_y2 - roi_y1)) + roi_y1\n",
    "                cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "            if collecting and current_gesture:\n",
    "                db.save_sample(current_gesture, landmarks)\n",
    "                sample_count += 1\n",
    "\n",
    "                if sample_count >= target_samples:\n",
    "                    print(f\"âœ… Collected {sample_count} samples for {current_gesture}\")\n",
    "                    collecting = False\n",
    "                    current_gesture = None\n",
    "                    sample_count = 0\n",
    "\n",
    "        # UI\n",
    "        cv2.rectangle(frame, (roi_x1, roi_y1), (roi_x2, roi_y2), (0, 255, 0), 3)\n",
    "        cv2.rectangle(frame, (0, 0), (w, 100), (0, 0, 0), -1)\n",
    "\n",
    "        if collecting and current_gesture:\n",
    "            status_text = f\"COLLECTING: {current_gesture.upper()}\"\n",
    "            color = (0, 255, 0)\n",
    "            progress = f\"{sample_count}/{target_samples}\"\n",
    "            cv2.putText(frame, progress, (20, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "        else:\n",
    "            status_text = \"SELECT GESTURE (1-6)\"\n",
    "            color = (100, 100, 100)\n",
    "\n",
    "        cv2.putText(frame, status_text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
    "\n",
    "        hand_status = \"HAND DETECTED\" if landmarks else \"NO HAND\"\n",
    "        hand_color = (0, 255, 0) if landmarks else (0, 0, 255)\n",
    "        cv2.putText(frame, hand_status, (400, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, hand_color, 2)\n",
    "\n",
    "        cv2.imshow(\"Data Collection\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('c'):\n",
    "            if collecting:\n",
    "                print(f\"â¸ï¸  Stopped collecting {current_gesture} (saved {sample_count} samples)\")\n",
    "            collecting = False\n",
    "            current_gesture = None\n",
    "            sample_count = 0\n",
    "        elif key == ord('s'):\n",
    "            stats = db.get_sample_count()\n",
    "            print(\"\\n\" + \"=\"*40)\n",
    "            print(\"ðŸ“Š DATABASE STATISTICS\")\n",
    "            print(\"=\"*40)\n",
    "            for gesture, count in stats.items():\n",
    "                print(f\"  {gesture.upper():15s}: {count:4d} samples\")\n",
    "            print(\"=\"*40 + \"\\n\")\n",
    "        elif chr(key) in gestures:\n",
    "            current_gesture = gestures[chr(key)]\n",
    "            collecting = True\n",
    "            sample_count = 0\n",
    "            print(f\"\\nâ–¶ï¸  Collecting '{current_gesture}' (target: {target_samples})\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Final stats\n",
    "    stats = db.get_sample_count()\n",
    "    total = sum(stats.values())\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ðŸ“Š FINAL STATISTICS\")\n",
    "    print(\"=\"*40)\n",
    "    for gesture, count in stats.items():\n",
    "        print(f\"  {gesture.upper():15s}: {count:4d} samples\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"  TOTAL:          {total:4d} samples\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "    db.close()\n",
    "\n",
    "# =========================================================\n",
    "# TRAINING MODULE\n",
    "# =========================================================\n",
    "def train_model():\n",
    "    \"\"\"Model training interface\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ“ MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    db = GestureDatabase()\n",
    "\n",
    "    stats = db.get_sample_count()\n",
    "\n",
    "    if not stats:\n",
    "        print(\"\\nâŒ No samples found!\")\n",
    "        print(\"Run data collection first (option 1).\")\n",
    "        db.close()\n",
    "        return\n",
    "\n",
    "    total = sum(stats.values())\n",
    "    print(\"\\nðŸ“Š Database Statistics:\")\n",
    "    for gesture, count in stats.items():\n",
    "        print(f\"  {gesture.upper():15s}: {count:4d} samples\")\n",
    "    print(f\"\\n  TOTAL: {total}\")\n",
    "\n",
    "    if total < 50:\n",
    "        print(\"\\nâš ï¸  WARNING: Low sample count!\")\n",
    "        response = input(\"Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            db.close()\n",
    "            return\n",
    "\n",
    "    print(\"\\nðŸ“¥ Loading samples...\")\n",
    "    samples = db.get_samples()\n",
    "\n",
    "    num_classes = len(stats)\n",
    "    cnn = GestureCNN(num_classes=num_classes, input_shape=(21, 3))\n",
    "\n",
    "    print(\"ðŸ”„ Preparing data...\")\n",
    "    X, y = cnn.prepare_data(samples)\n",
    "\n",
    "    print(\"ðŸ—ï¸  Building model...\")\n",
    "    cnn.build_model()\n",
    "    print(cnn.model.summary())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING PARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    epochs = int(input(\"Epochs (default 50): \") or \"50\")\n",
    "    batch_size = int(input(\"Batch size (default 32): \") or \"32\")\n",
    "    validation_split = float(input(\"Validation split (default 0.2): \") or \"0.2\")\n",
    "\n",
    "    print(f\"\\n  Epochs: {epochs}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Validation split: {validation_split}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"\\nðŸŽ“ Training...\\n\")\n",
    "    history = cnn.train(X, y, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    final_accuracy = history.history['val_accuracy'][-1]\n",
    "    final_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    print(\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Validation Accuracy: {final_accuracy*100:.2f}%\")\n",
    "    print(f\"  Validation Loss: {final_loss:.4f}\")\n",
    "\n",
    "    model_path = \"gesture_cnn_model.h5\"\n",
    "    print(f\"\\nðŸ’¾ Saving to '{model_path}'...\")\n",
    "    cnn.save_model(model_path)\n",
    "\n",
    "    db.save_training_session(model_path, final_accuracy, final_loss, epochs, total)\n",
    "    db.close()\n",
    "\n",
    "    print(\"\\nâœ… Training complete!\\n\")\n",
    "\n",
    "# =========================================================\n",
    "# REAL-TIME CONTROL MODULE\n",
    "# =========================================================\n",
    "def run_control():\n",
    "    \"\"\"Real-time gesture control interface\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš GESTURE CONTROL MODE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model_path = \"gesture_cnn_model.h5\"\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\nâŒ Model not found!\")\n",
    "        print(\"Train the model first (option 2).\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        recognizer = HybridGestureRecognizer(use_cnn=True, model_path=model_path)\n",
    "        print(\"âœ… CNN model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    db = GestureDatabase()\n",
    "    gesture_names = db.get_all_gesture_names()\n",
    "\n",
    "    print(\"\\nðŸ“‹ Available Gestures:\")\n",
    "    for name in gesture_names:\n",
    "        print(f\"  â€¢ {name.upper()}\")\n",
    "\n",
    "    print(\"\\nâš™ï¸  Controls:\")\n",
    "    print(\"  r: Reset buffer\")\n",
    "    print(\"  q: Quit\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    current_command = \"none\"\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    fps = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        roi_x1, roi_y1 = 120, 60\n",
    "        roi_x2, roi_y2 = 520, 420\n",
    "        roi = frame[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "\n",
    "        landmarks = recognizer.detect_hand(roi)\n",
    "\n",
    "        raw_gesture = \"none\"\n",
    "        confidence = 0.0\n",
    "\n",
    "        if landmarks:\n",
    "            raw_gesture, confidence = recognizer.recognize_gesture(landmarks)\n",
    "\n",
    "            # Draw landmarks\n",
    "            for landmark in landmarks:\n",
    "                x = int(landmark.x * (roi_x2 - roi_x1)) + roi_x1\n",
    "                y = int(landmark.y * (roi_y2 - roi_y1)) + roi_y1\n",
    "                cv2.circle(frame, (x, y), 5, (0, 255, 255), -1)\n",
    "\n",
    "            # Draw connections\n",
    "            connections = [\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "                (5, 9), (9, 13), (13, 17)\n",
    "            ]\n",
    "\n",
    "            for start, end in connections:\n",
    "                x1 = int(landmarks[start].x * (roi_x2 - roi_x1)) + roi_x1\n",
    "                y1 = int(landmarks[start].y * (roi_y2 - roi_y1)) + roi_y1\n",
    "                x2 = int(landmarks[end].x * (roi_x2 - roi_x1)) + roi_x1\n",
    "                y2 = int(landmarks[end].y * (roi_y2 - roi_y1)) + roi_y1\n",
    "                cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        stable = recognizer.stable_gesture(raw_gesture)\n",
    "\n",
    "        if stable != \"none\" and stable != current_command:\n",
    "            if recognizer.can_send():\n",
    "                current_command = stable\n",
    "                recognizer.mark_sent()\n",
    "\n",
    "                if current_command in COMMAND_MAP:\n",
    "                    COMMAND_MAP[current_command]()\n",
    "                    print(f\"âš¡ {current_command.upper()} (confidence: {confidence*100:.1f}%)\")\n",
    "\n",
    "        # FPS calculation\n",
    "        frame_count += 1\n",
    "        if frame_count % 30 == 0:\n",
    "            end_time = time.time()\n",
    "            fps = 30 / (end_time - start_time)\n",
    "            start_time = time.time()\n",
    "\n",
    "        # UI\n",
    "        roi_color = (0, 255, 0) if recognizer.can_send() else (0, 165, 255)\n",
    "        cv2.rectangle(frame, (roi_x1, roi_y1), (roi_x2, roi_y2), roi_color, 3)\n",
    "\n",
    "        cv2.rectangle(frame, (0, 0), (w, 150), (0, 0, 0), -1)\n",
    "\n",
    "        hand_status = \"HAND DETECTED\" if landmarks else \"NO HAND\"\n",
    "        hand_color = (0, 255, 0) if landmarks else (100, 100, 100)\n",
    "        cv2.putText(frame, hand_status, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, hand_color, 2)\n",
    "\n",
    "        if raw_gesture != \"none\":\n",
    "            raw_text = f\"RAW: {raw_gesture.upper()} ({confidence*100:.1f}%)\"\n",
    "            cv2.putText(frame, raw_text, (20, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        cmd_color = (0, 255, 0) if current_command != \"none\" else (100, 100, 100)\n",
    "        cv2.putText(frame, f\"CMD: {current_command.upper()}\", (20, 120),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, cmd_color, 3)\n",
    "\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (w - 150, 40),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "\n",
    "        status = \"READY\" if recognizer.can_send() else \"COOLDOWN\"\n",
    "        status_color = (0, 255, 0) if recognizer.can_send() else (0, 165, 255)\n",
    "        cv2.putText(frame, status, (w - 150, 75),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)\n",
    "\n",
    "        # Stability bar\n",
    "        if raw_gesture != \"none\":\n",
    "            stable_count = sum(1 for g in recognizer.buffer if g == raw_gesture)\n",
    "\n",
    "            bar_x, bar_y = 20, h - 40\n",
    "            bar_w, bar_h = 300, 20\n",
    "\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_w, bar_y + bar_h), (50, 50, 50), -1)\n",
    "\n",
    "            fill_w = int((stable_count / recognizer.confirmation_threshold) * bar_w)\n",
    "            bar_color = (0, 255, 0) if stable_count >= recognizer.confirmation_threshold else (0, 165, 255)\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), (bar_x + fill_w, bar_y + bar_h), bar_color, -1)\n",
    "\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_w, bar_y + bar_h), (255, 255, 255), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"Stability: {stable_count}/{recognizer.confirmation_threshold}\",\n",
    "                       (bar_x, bar_y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "        cv2.imshow(\"Gesture Control\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            recognizer.buffer.clear()\n",
    "            current_command = \"none\"\n",
    "            print(\"ðŸ”„ Buffer reset\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    db.close()\n",
    "\n",
    "    print(\"\\nâœ… Control mode closed\\n\")\n",
    "\n",
    "# =========================================================\n",
    "# MAIN MENU\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš CNN GESTURE RECOGNITION SYSTEM ðŸš\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nAll-in-one system for gesture-based drone control\")\n",
    "    print(\"Combines MediaPipe + CNN + SQLite Database\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MAIN MENU\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1. Collect Training Data\")\n",
    "        print(\"2. Train CNN Model\")\n",
    "        print(\"3. Run Gesture Control\")\n",
    "        print(\"4. View Database Statistics\")\n",
    "        print(\"5. Clear Database\")\n",
    "        print(\"6. Exit\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        choice = input(\"\\nSelect option (1-6): \").strip()\n",
    "\n",
    "        if choice == '1':\n",
    "            collect_data()\n",
    "\n",
    "        elif choice == '2':\n",
    "            train_model()\n",
    "\n",
    "        elif choice == '3':\n",
    "            run_control()\n",
    "\n",
    "        elif choice == '4':\n",
    "            db = GestureDatabase()\n",
    "            stats = db.get_sample_count()\n",
    "\n",
    "            if stats:\n",
    "                total = sum(stats.values())\n",
    "                print(\"\\n\" + \"=\"*40)\n",
    "                print(\"ðŸ“Š DATABASE STATISTICS\")\n",
    "                print(\"=\"*40)\n",
    "                for gesture, count in stats.items():\n",
    "                    print(f\"  {gesture.upper():15s}: {count:4d} samples\")\n",
    "                print(\"=\"*40)\n",
    "                print(f\"  TOTAL:          {total:4d} samples\")\n",
    "                print(\"=\"*40)\n",
    "            else:\n",
    "                print(\"\\nðŸ“Š No data in database\")\n",
    "\n",
    "            db.close()\n",
    "\n",
    "        elif choice == '5':\n",
    "            print(\"\\nâš ï¸  WARNING: This will delete all training data!\")\n",
    "            confirm = input(\"Type 'DELETE' to confirm: \").strip()\n",
    "\n",
    "            if confirm == 'DELETE':\n",
    "                db = GestureDatabase()\n",
    "                db.clear_samples()\n",
    "                db.close()\n",
    "                print(\"ðŸ—‘ï¸  All samples deleted\")\n",
    "            else:\n",
    "                print(\"âŒ Cancelled\")\n",
    "\n",
    "        elif choice == '6':\n",
    "            print(\"\\nðŸ‘‹ Goodbye!\\n\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"\\nâŒ Invalid option. Please select 1-6.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for MediaPipe model\n",
    "    if not os.path.exists(\"hand_landmarker.task\"):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âš ï¸  WARNING: hand_landmarker.task not found!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nPlease download the MediaPipe hand landmark model:\")\n",
    "        print(\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\")\n",
    "        print(\"\\nPlace 'hand_landmarker.task' in the same folder as this script.\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        input(\"Press Enter after downloading the file...\")\n",
    "\n",
    "    main()"
   ],
   "id": "fafae539598cbf76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš CNN GESTURE RECOGNITION SYSTEM ðŸš\n",
      "============================================================\n",
      "\n",
      "All-in-one system for gesture-based drone control\n",
      "Combines MediaPipe + CNN + SQLite Database\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MAIN MENU\n",
      "============================================================\n",
      "1. Collect Training Data\n",
      "2. Train CNN Model\n",
      "3. Run Gesture Control\n",
      "4. View Database Statistics\n",
      "5. Clear Database\n",
      "6. Exit\n",
      "============================================================\n",
      "\n",
      "========================================\n",
      "ðŸ“Š DATABASE STATISTICS\n",
      "========================================\n",
      "  HOVER          :  500 samples\n",
      "  LEFT           :  852 samples\n",
      "  RIGHT          :  800 samples\n",
      "  STOP           :  500 samples\n",
      "  SUMMERSAULT    :  600 samples\n",
      "  UP             :  700 samples\n",
      "========================================\n",
      "  TOTAL:          3952 samples\n",
      "========================================\n",
      "\n",
      "============================================================\n",
      "MAIN MENU\n",
      "============================================================\n",
      "1. Collect Training Data\n",
      "2. Train CNN Model\n",
      "3. Run Gesture Control\n",
      "4. View Database Statistics\n",
      "5. Clear Database\n",
      "6. Exit\n",
      "============================================================\n",
      "\n",
      "ðŸ‘‹ Goodbye!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a07d0d9c8f1f9b02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
